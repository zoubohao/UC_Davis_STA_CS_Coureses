{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.1225\n",
      "Epoch [1/5], Step [200/600], Loss: 0.0476\n",
      "Epoch [1/5], Step [300/600], Loss: 0.0754\n",
      "Epoch [1/5], Step [400/600], Loss: 0.0599\n",
      "Epoch [1/5], Step [500/600], Loss: 0.0237\n",
      "Epoch [1/5], Step [600/600], Loss: 0.1151\n",
      "Epoch [2/5], Step [100/600], Loss: 0.0184\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0182\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0289\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0490\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0186\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0972\n",
      "Epoch [3/5], Step [100/600], Loss: 0.1134\n",
      "Epoch [3/5], Step [200/600], Loss: 0.1508\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0312\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0782\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0990\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0562\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0102\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0149\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0437\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0206\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0586\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0460\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0276\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0320\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0052\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0028\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0072\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0264\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "model = ConvNet(num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 99.07 %\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "# Test the model\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN for generating text \n",
    "https://towardsdatascience.com/writing-like-shakespeare-with-machine-learning-in-pytorch-d77f851d910c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Open shakespeare text file and read in data as `text`\n",
    "with open('data/shakespeare.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is the 100th Etext file presented by Project Gutenberg, and\\nis presented in cooperation with Wo'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing the first 100 characters\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 68, 10, 85, 77, 10, 85, 77, 49, 68, 27, 77,  0, 65, 65, 49, 68,\n",
       "       77, 56, 49, 27, 41, 49, 77, 48, 10, 24, 27, 77, 78, 63, 27, 85, 27,\n",
       "       74, 49, 27, 87, 77, 33, 50, 77, 13, 63, 75, 54, 27, 43, 49, 77, 35,\n",
       "       59, 49, 27, 74, 33, 27, 63, 62, 89, 77, 79, 74, 87,  9, 10, 85, 77,\n",
       "       78, 63, 27, 85, 27, 74, 49, 27, 87, 77, 10, 74, 77, 43, 75, 75, 78,\n",
       "       27, 63, 79, 49, 10, 75, 74, 77, 18, 10, 49, 68, 77, 52, 75])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding the text and map each character to an integer and vice versa\n",
    "\n",
    "# We create two dictionaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to integers\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# Encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "\n",
    "# Showing the first 100 encoded characters\n",
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot\n",
    "    \n",
    "# Defining method to make mini-batches for training\n",
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU; consider making n_epochs very small.\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the model\n",
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        #define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        #define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        #define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        #get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        #pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        #put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(91, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=91, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Declaring the train method\n",
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "                      \n",
    "# Define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5... Step: 50... Loss: 3.2490... Val Loss: 3.2136\n",
      "Epoch: 1/5... Step: 100... Loss: 3.1598... Val Loss: 3.1830\n",
      "Epoch: 1/5... Step: 150... Loss: 2.8971... Val Loss: 2.8927\n",
      "Epoch: 1/5... Step: 200... Loss: 2.6111... Val Loss: 2.5472\n",
      "Epoch: 1/5... Step: 250... Loss: 2.3613... Val Loss: 2.3088\n",
      "Epoch: 1/5... Step: 300... Loss: 2.2522... Val Loss: 2.1792\n",
      "Epoch: 1/5... Step: 350... Loss: 2.1404... Val Loss: 2.0816\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-24cef111b708>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Saving the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-a9c391d97b2e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, data, epochs, batch_size, seq_length, lr, clip, val_frac, print_every)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# calculate the loss and perform backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;31m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Declaring the hyperparameters\n",
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 5 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=50)\n",
    "\n",
    "# Saving the model\n",
    "model_name = 'rnn_20_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)\n",
    "    \n",
    "# Defining a method to generate the next character\n",
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h\n",
    "        \n",
    "# Declaring a method to generate new text\n",
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating new text\n",
    "print(sample(net, 1000, prime='A', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, I had to interrupt the process because it took very long! Perhaps if you are curious, you could try this code on the server and get the results. Or take a look at the results generated in the website provided above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib_is_available = True\n",
    "try:\n",
    "  from matplotlib import pyplot as plt\n",
    "except ImportError:\n",
    "  print(\"Will skip plotting; matplotlib is not available.\")\n",
    "  matplotlib_is_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data params\n",
    "data_mean = 4\n",
    "data_stddev = 1.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Uncomment only one of these to define what data is actually sent to the Discriminator\n",
    "#(name, preprocess, d_input_func) = (\"Raw data\", lambda data: data, lambda x: x)\n",
    "#(name, preprocess, d_input_func) = (\"Data and variances\", lambda data: decorate_with_diffs(data, 2.0), lambda x: x * 2)\n",
    "#(name, preprocess, d_input_func) = (\"Data and diffs\", lambda data: decorate_with_diffs(data, 1.0), lambda x: x * 2)\n",
    "(name, preprocess, d_input_func) = (\"Only 4 moments\", lambda data: get_moments(data), lambda x: 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data [Only 4 moments]\n"
     ]
    }
   ],
   "source": [
    "print(\"Using data [%s]\" % (name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution_sampler(mu, sigma):\n",
    "    return lambda n: torch.Tensor(np.random.normal(mu, sigma, (1, n)))  # Gaussian\n",
    "\n",
    "def get_generator_input_sampler():\n",
    "    return lambda m, n: torch.rand(m, n)  # Uniform-dist data into generator, _NOT_ Gaussian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, f):\n",
    "        super(Generator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.map1(x)\n",
    "        x = self.f(x)\n",
    "        x = self.map2(x)\n",
    "        x = self.f(x)\n",
    "        x = self.map3(x)\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, f):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f(self.map1(x))\n",
    "        x = self.f(self.map2(x))\n",
    "        return self.f(self.map3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(v):\n",
    "    return v.data.storage().tolist()\n",
    "\n",
    "def stats(d):\n",
    "    return [np.mean(d), np.std(d)]\n",
    "\n",
    "def get_moments(d):\n",
    "    # Return the first 4 moments of the data provided\n",
    "    mean = torch.mean(d)\n",
    "    diffs = d - mean\n",
    "    var = torch.mean(torch.pow(diffs, 2.0))\n",
    "    std = torch.pow(var, 0.5)\n",
    "    zscores = diffs / std\n",
    "    skews = torch.mean(torch.pow(zscores, 3.0))\n",
    "    kurtoses = torch.mean(torch.pow(zscores, 4.0)) - 3.0  # excess kurtosis, should be 0 for Gaussian\n",
    "    final = torch.cat((mean.reshape(1,), std.reshape(1,), skews.reshape(1,), kurtoses.reshape(1,)))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorate_with_diffs(data, exponent, remove_raw_data=False):\n",
    "    mean = torch.mean(data.data, 1, keepdim=True)\n",
    "    mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[0][0])\n",
    "    diffs = torch.pow(data - Variable(mean_broadcast), exponent)\n",
    "    if remove_raw_data:\n",
    "        return torch.cat([diffs], 1)\n",
    "    else:\n",
    "        return torch.cat([data, diffs], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:512: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: D (0.6358835697174072 real_err, 0.7440698146820068 fake_err) G (0.6455076932907104 err); Real Dist ([3.985840771198273, 1.2675471781292351]),  Fake Dist ([0.4671567519903183, 0.0033515308554551973]) \n",
      "Epoch 100: D (0.5660937428474426 real_err, 0.5849903225898743 fake_err) G (0.8124746084213257 err); Real Dist ([4.029280935108662, 1.2393676642988891]),  Fake Dist ([2.7715966992378234, 0.005839416731167195]) \n",
      "Epoch 200: D (0.3237815201282501 real_err, 0.6025383472442627 fake_err) G (0.3204660415649414 err); Real Dist ([4.004914841830731, 1.2540001957698945]),  Fake Dist ([0.6617648614645004, 0.9032486316563701]) \n",
      "Epoch 300: D (0.7155933380126953 real_err, 1.025075078010559 fake_err) G (0.3850141167640686 err); Real Dist ([3.963545500040054, 1.2446940155738904]),  Fake Dist ([2.06364945435524, 0.17597738897138593]) \n",
      "Epoch 400: D (0.6410775780677795 real_err, 0.5808625221252441 fake_err) G (0.7881188988685608 err); Real Dist ([3.9642810295820237, 1.257673872921598]),  Fake Dist ([7.4332388062477115, 1.8791574218452043]) \n",
      "Epoch 500: D (0.2745770215988159 real_err, 0.2409825325012207 fake_err) G (1.5124645233154297 err); Real Dist ([3.9742575996816156, 1.2464180856815823]),  Fake Dist ([4.011804111003876, 0.45201110203732986]) \n",
      "Epoch 600: D (0.14025555551052094 real_err, 0.11218339949846268 fake_err) G (0.6535581946372986 err); Real Dist ([3.948605976320803, 1.265373595382838]),  Fake Dist ([10.268430795669556, 3.037538359411135]) \n",
      "Epoch 700: D (0.690475583076477 real_err, 0.6816068291664124 fake_err) G (0.6937621235847473 err); Real Dist ([4.0450697362422945, 1.2934568519406293]),  Fake Dist ([3.942719172000885, 1.1987111638245707]) \n",
      "Epoch 800: D (0.681056559085846 real_err, 0.713944137096405 fake_err) G (0.7095291614532471 err); Real Dist ([4.090102675199509, 1.3044872823944098]),  Fake Dist ([3.9936227979660033, 1.2485654214800945]) \n",
      "Epoch 900: D (0.7000597715377808 real_err, 0.7028559446334839 fake_err) G (0.7076801657676697 err); Real Dist ([4.013189597845077, 1.2328295967219056]),  Fake Dist ([3.985049773454666, 1.2175102596667406]) \n",
      "Epoch 1000: D (0.6899158954620361 real_err, 0.6926175355911255 fake_err) G (0.6883677244186401 err); Real Dist ([3.971122584104538, 1.1424991445272383]),  Fake Dist ([3.9238787455558777, 1.2756997928797904]) \n",
      "Epoch 1100: D (0.6922139525413513 real_err, 0.696653425693512 fake_err) G (0.6854892373085022 err); Real Dist ([3.8667801852822303, 1.2110174944873189]),  Fake Dist ([4.125750278234482, 1.2925196388426312]) \n",
      "Epoch 1200: D (0.6887549161911011 real_err, 0.6976730823516846 fake_err) G (0.6988551616668701 err); Real Dist ([4.042023239195347, 1.2399985774518756]),  Fake Dist ([3.855653546333313, 1.2265411473880627]) \n",
      "Epoch 1300: D (0.6995164752006531 real_err, 0.695929229259491 fake_err) G (0.6911148428916931 err); Real Dist ([4.0164708008766175, 1.2666651737348622]),  Fake Dist ([3.9934384994506837, 1.2596269770000996]) \n",
      "Epoch 1400: D (0.6905730962753296 real_err, 0.690769374370575 fake_err) G (0.693642258644104 err); Real Dist ([3.9149411039054396, 1.255108769052013]),  Fake Dist ([3.8464254195690155, 1.2358614513935486]) \n",
      "Epoch 1500: D (0.6938707232475281 real_err, 0.6960988640785217 fake_err) G (0.6924811601638794 err); Real Dist ([3.9983724949359893, 1.1920107632963783]),  Fake Dist ([3.9227418792247772, 1.2506492499433564]) \n",
      "Epoch 1600: D (0.6973305344581604 real_err, 0.6947334408760071 fake_err) G (0.6923249959945679 err); Real Dist ([4.004772542417049, 1.2309308431101194]),  Fake Dist ([4.016893958330154, 1.2596229269961885]) \n",
      "Epoch 1700: D (0.6922879219055176 real_err, 0.695567786693573 fake_err) G (0.6946057081222534 err); Real Dist ([3.95247898876667, 1.1513329434698647]),  Fake Dist ([4.008859547615051, 1.24340925683438]) \n",
      "Epoch 1800: D (0.6968319416046143 real_err, 0.6939704418182373 fake_err) G (0.6908769011497498 err); Real Dist ([3.9714942438453438, 1.2395152159087142]),  Fake Dist ([3.983775064706802, 1.2203138385832728]) \n",
      "Epoch 1900: D (0.6926816701889038 real_err, 0.6915484070777893 fake_err) G (0.6943144798278809 err); Real Dist ([3.918466155469418, 1.2074075389798575]),  Fake Dist ([4.039923120975494, 1.1606251932100338]) \n",
      "Epoch 2000: D (0.690912127494812 real_err, 0.694891631603241 fake_err) G (0.6911011338233948 err); Real Dist ([4.00108699452877, 1.2298682922936457]),  Fake Dist ([3.9280596084594728, 1.22790209848866]) \n",
      "Epoch 2100: D (0.6913745403289795 real_err, 0.6937883496284485 fake_err) G (0.691689133644104 err); Real Dist ([3.9871495952904223, 1.2457632551766804]),  Fake Dist ([3.974574322938919, 1.2799840706647176]) \n",
      "Epoch 2200: D (0.6953256130218506 real_err, 0.6935831308364868 fake_err) G (0.6919747591018677 err); Real Dist ([4.006099713742733, 1.260988975853081]),  Fake Dist ([3.9912931351661682, 1.2689035184183293]) \n",
      "Epoch 2300: D (0.695469081401825 real_err, 0.6955924034118652 fake_err) G (0.6923608183860779 err); Real Dist ([3.954683602005243, 1.220612880969763]),  Fake Dist ([3.8961462903022768, 1.2629411592233992]) \n",
      "Epoch 2400: D (0.6918185353279114 real_err, 0.6946786642074585 fake_err) G (0.6922813653945923 err); Real Dist ([4.030594045996666, 1.2551649198191317]),  Fake Dist ([4.034082663059235, 1.2398681131701221]) \n",
      "Epoch 2500: D (0.691308856010437 real_err, 0.6946509480476379 fake_err) G (0.691191554069519 err); Real Dist ([4.005889185070991, 1.227912633763616]),  Fake Dist ([3.999866503715515, 1.2715666690297178]) \n",
      "Epoch 2600: D (0.6912949681282043 real_err, 0.6945857405662537 fake_err) G (0.6920331716537476 err); Real Dist ([3.84543439912796, 1.2641797628165128]),  Fake Dist ([4.055676463842392, 1.2616226921070732]) \n",
      "Epoch 2700: D (0.6985535025596619 real_err, 0.6939597129821777 fake_err) G (0.6936684250831604 err); Real Dist ([3.915370205760002, 1.2736954274341916]),  Fake Dist ([3.995987603187561, 1.1911919500388146]) \n",
      "Epoch 2800: D (0.6919584274291992 real_err, 0.6934201121330261 fake_err) G (0.6933769583702087 err); Real Dist ([3.9406616351008417, 1.2763909463496326]),  Fake Dist ([4.040728610277176, 1.181927771549095]) \n",
      "Epoch 2900: D (0.6931701898574829 real_err, 0.6930636167526245 fake_err) G (0.6921684741973877 err); Real Dist ([3.8885968443155288, 1.1800459927428995]),  Fake Dist ([3.9679856221675873, 1.2686384515817277]) \n",
      "Epoch 3000: D (0.6925635933876038 real_err, 0.693840742111206 fake_err) G (0.692730724811554 err); Real Dist ([4.027162852168083, 1.2633277729289434]),  Fake Dist ([4.085156988620758, 1.2952944024645627]) \n",
      "Epoch 3100: D (0.6944858431816101 real_err, 0.6932743787765503 fake_err) G (0.692690372467041 err); Real Dist ([4.066078728556633, 1.2115058133387717]),  Fake Dist ([3.9774448256492616, 1.1721684005551942]) \n",
      "Epoch 3200: D (0.6926659345626831 real_err, 0.6919215321540833 fake_err) G (0.692902684211731 err); Real Dist ([4.045098998606205, 1.2995971946421379]),  Fake Dist ([4.15328898024559, 1.2174146019263505]) \n",
      "Epoch 3300: D (0.692410409450531 real_err, 0.6933512687683105 fake_err) G (0.6940968632698059 err); Real Dist ([4.008001916328445, 1.240538492206849]),  Fake Dist ([4.050724298000335, 1.2267038337138783]) \n",
      "Epoch 3400: D (0.6923651099205017 real_err, 0.693317174911499 fake_err) G (0.6923820376396179 err); Real Dist ([4.003671039775014, 1.2753419855778059]),  Fake Dist ([3.97376758813858, 1.2924383280319807]) \n",
      "Epoch 3500: D (0.6933152675628662 real_err, 0.6930830478668213 fake_err) G (0.6926870346069336 err); Real Dist ([4.08436548268795, 1.3352502363867516]),  Fake Dist ([3.923574290037155, 1.2327616882877672]) \n",
      "Epoch 3600: D (0.6927470564842224 real_err, 0.6937583088874817 fake_err) G (0.6931878328323364 err); Real Dist ([4.038798459649086, 1.2277409933658026]),  Fake Dist ([3.936778071641922, 1.238145779785067]) \n",
      "Epoch 3700: D (0.6917649507522583 real_err, 0.6933556795120239 fake_err) G (0.6925593018531799 err); Real Dist ([3.9978011770248414, 1.200534120667582]),  Fake Dist ([3.866106190919876, 1.2779436636464463]) \n",
      "Epoch 3800: D (0.6928505301475525 real_err, 0.6920138001441956 fake_err) G (0.6924126148223877 err); Real Dist ([3.9951076668500902, 1.1948450192140625]),  Fake Dist ([4.023792816638947, 1.1791221400306737]) \n",
      "Epoch 3900: D (0.6952357292175293 real_err, 0.693366289138794 fake_err) G (0.6933853030204773 err); Real Dist ([3.973938076257706, 1.261823910820605]),  Fake Dist ([3.930084249973297, 1.2620086827009573]) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4000: D (0.6927763819694519 real_err, 0.6933629512786865 fake_err) G (0.6932995319366455 err); Real Dist ([3.989915632545948, 1.3034143899230135]),  Fake Dist ([4.061218119144439, 1.2722264230429203]) \n",
      "Epoch 4100: D (0.6925982236862183 real_err, 0.6934898495674133 fake_err) G (0.6934146881103516 err); Real Dist ([3.966803432777524, 1.2482069361780825]),  Fake Dist ([3.997150721311569, 1.1903797890023975]) \n",
      "Epoch 4200: D (0.6922729015350342 real_err, 0.6938718557357788 fake_err) G (0.6925795674324036 err); Real Dist ([3.997735216259956, 1.2266235428578305]),  Fake Dist ([3.923613603591919, 1.2716865626169098]) \n",
      "Epoch 4300: D (0.6947535872459412 real_err, 0.6906834244728088 fake_err) G (0.6945594549179077 err); Real Dist ([4.095552872896194, 1.27868310937019]),  Fake Dist ([4.042745877504348, 1.2089741169738606]) \n",
      "Epoch 4400: D (0.6931111812591553 real_err, 0.6940391659736633 fake_err) G (0.6926850080490112 err); Real Dist ([4.067909948244691, 1.3037868764770868]),  Fake Dist ([4.069843422412872, 1.2406829385477516]) \n",
      "Epoch 4500: D (0.6941536068916321 real_err, 0.6927321553230286 fake_err) G (0.6929264068603516 err); Real Dist ([3.9955260205343364, 1.332849621329825]),  Fake Dist ([3.988787106513977, 1.2183209099557382]) \n",
      "Epoch 4600: D (0.6928941607475281 real_err, 0.6934829354286194 fake_err) G (0.693077564239502 err); Real Dist ([4.001868417918682, 1.2344733890894084]),  Fake Dist ([3.9400190811157225, 1.2451963363146412]) \n",
      "Epoch 4700: D (0.6928507685661316 real_err, 0.6934265494346619 fake_err) G (0.6928638815879822 err); Real Dist ([4.030562645196914, 1.2884114147183174]),  Fake Dist ([4.003772645950318, 1.2625621220799643]) \n",
      "Epoch 4800: D (0.6927177906036377 real_err, 0.6925868391990662 fake_err) G (0.6928969621658325 err); Real Dist ([3.9689838831871747, 1.2612637365347397]),  Fake Dist ([3.917761614084244, 1.2699553116111484]) \n",
      "Epoch 4900: D (0.6952716112136841 real_err, 0.6938729286193848 fake_err) G (0.6926730871200562 err); Real Dist ([3.9791069062352182, 1.1530571599818]),  Fake Dist ([3.955293289422989, 1.2603181943260753]) \n",
      "Plotting the generated distribution...\n",
      " Values: [4.100412845611572, 2.7563552856445312, 3.4747180938720703, 3.8974547386169434, 2.6475868225097656, 4.125875473022461, 3.80629825592041, 4.018919467926025, 1.4924381971359253, 3.5740013122558594, 2.7656326293945312, 3.6202378273010254, 4.365547180175781, 3.811939239501953, 4.514886379241943, 5.687680244445801, 3.5487241744995117, 3.9913740158081055, 1.8730204105377197, 2.0646800994873047, 4.059211254119873, 3.165694236755371, 3.6875391006469727, 4.233626842498779, 3.7150869369506836, 3.7725324630737305, 2.755824089050293, 4.220559120178223, 1.6773711442947388, 1.8199485540390015, 2.265352964401245, 5.573733329772949, 3.6263198852539062, 4.126986026763916, 2.30954647064209, 1.1798313856124878, 5.077178478240967, 3.639387607574463, 4.035075664520264, 1.835292100906372, 5.612241744995117, 4.535651683807373, 3.960153579711914, 5.892780303955078, 3.0517475605010986, 4.247141361236572, 3.539022445678711, 4.454439640045166, 4.648491859436035, 4.394906520843506, 4.447458744049072, 3.9740757942199707, 3.998136043548584, 4.082025527954102, 2.8456051349639893, 4.696990489959717, 6.046105861663818, 3.1412248611450195, 3.3580880165100098, 6.413827896118164, 3.7784905433654785, 2.538917303085327, 4.832817077636719, 1.6637617349624634, 1.9023553133010864, 3.042850971221924, 5.004656791687012, 4.747670650482178, 5.321516036987305, 1.72939133644104, 1.6278384923934937, 3.5508246421813965, 3.073019504547119, 2.7046501636505127, 4.495916843414307, 5.5608344078063965, 5.3312087059021, 4.9189887046813965, 4.083347797393799, 4.191242694854736, 2.036616325378418, 4.249813079833984, 2.4228670597076416, 2.356945276260376, 3.589043617248535, 4.87155818939209, 4.209907531738281, 5.372422218322754, 3.1903743743896484, 5.080796241760254, 3.019685745239258, 3.209853172302246, 2.298649311065674, 4.452420711517334, 3.0433173179626465, 5.051260471343994, 2.540433406829834, 1.8245940208435059, 1.5915039777755737, 2.8406028747558594, 3.8505544662475586, 2.158290386199951, 7.152761936187744, 4.093930244445801, 5.954333305358887, 2.5248873233795166, 3.3559093475341797, 4.251674652099609, 4.994400501251221, 5.336554050445557, 5.354898452758789, 3.8030447959899902, 3.7656965255737305, 5.306453704833984, 6.677541255950928, 4.434966087341309, 3.988555908203125, 1.582963466644287, 4.5733256340026855, 4.233093738555908, 4.090112686157227, 4.680659770965576, 3.285397529602051, 4.070059776306152, 3.901580333709717, 4.324528217315674, 2.398070812225342, 4.934762477874756, 4.329868793487549, 3.9974312782287598, 4.460257053375244, 5.099516868591309, 4.035863876342773, 4.042385101318359, 6.688961505889893, 5.928450584411621, 3.9933133125305176, 3.6705079078674316, 4.3095550537109375, 3.745859146118164, 4.371936321258545, 4.45745849609375, 2.3550760746002197, 1.9223028421401978, 4.7773003578186035, 3.905734062194824, 5.815228462219238, 7.0785322189331055, 6.542926788330078, 5.84832763671875, 6.757342338562012, 2.7391443252563477, 4.7450737953186035, 5.475067138671875, 4.870571136474609, 6.245720863342285, 4.159904956817627, 2.5018444061279297, 4.733145713806152, 4.203629493713379, 3.616124153137207, 4.993373870849609, 4.776933670043945, 4.257896900177002, 6.2746992111206055, 3.3312158584594727, 5.227509021759033, 6.20681095123291, 3.6581168174743652, 3.228177070617676, 4.274353504180908, 4.115345001220703, 2.150179862976074, 3.8076491355895996, 1.6714733839035034, 3.765976905822754, 4.549020767211914, 4.361587047576904, 5.1909332275390625, 1.6255167722702026, 4.059412479400635, 1.6600900888442993, 4.549861431121826, 3.1910505294799805, 2.1413073539733887, 3.280764579772949, 6.973682403564453, 3.541717052459717, 4.39747428894043, 2.9825263023376465, 5.135702610015869, 4.916186809539795, 4.941972255706787, 3.7640886306762695, 4.605350494384766, 3.660886764526367, 2.2141482830047607, 4.7941741943359375, 4.023827075958252, 4.637331008911133, 3.6239237785339355, 4.859800338745117, 4.531569480895996, 4.688867568969727, 4.245035648345947, 5.906064033508301, 4.418079376220703, 4.473416805267334, 5.574262619018555, 1.6856361627578735, 1.9167168140411377, 1.5864249467849731, 3.970348834991455, 3.808156967163086, 4.265764236450195, 4.2084808349609375, 6.77595329284668, 4.552383899688721, 4.388312339782715, 5.274152755737305, 4.48953104019165, 4.887535572052002, 2.9725770950317383, 3.6024084091186523, 4.337477684020996, 1.8876618146896362, 7.008467674255371, 3.8939208984375, 5.641293525695801, 2.713047504425049, 2.761350631713867, 3.7523488998413086, 6.193072319030762, 5.691608428955078, 5.083871364593506, 3.4306063652038574, 4.820472717285156, 6.973724365234375, 4.986629962921143, 4.875596046447754, 1.351460576057434, 3.9400196075439453, 4.281928062438965, 1.60622239112854, 3.643354892730713, 4.006698131561279, 1.9702495336532593, 4.873541355133057, 6.065574645996094, 5.4443559646606445, 4.085994243621826, 4.871280670166016, 4.453883647918701, 4.2551798820495605, 7.087234973907471, 5.271108627319336, 2.8846793174743652, 4.324925899505615, 4.122285842895508, 3.9199352264404297, 4.417368412017822, 2.5683517456054688, 3.5706591606140137, 3.668799877166748, 2.947803020477295, 3.7309908866882324, 1.392905592918396, 3.966975212097168, 4.551176071166992, 4.248334884643555, 4.454429626464844, 2.2712135314941406, 1.3210526704788208, 3.536731719970703, 4.064174652099609, 4.530317783355713, 4.0232768058776855, 5.788046836853027, 4.214807510375977, 1.6349483728408813, 4.200476169586182, 4.394337177276611, 1.5563130378723145, 2.3254499435424805, 5.136969566345215, 6.898441314697266, 4.293608665466309, 2.0541839599609375, 4.484187126159668, 6.559008598327637, 6.575994491577148, 6.058239936828613, 2.4152307510375977, 3.9435296058654785, 2.2628626823425293, 3.2456698417663574, 1.685767650604248, 5.351678848266602, 4.754645347595215, 2.658968687057495, 2.351095676422119, 3.9883222579956055, 4.446691036224365, 3.688769817352295, 3.606381416320801, 4.6516242027282715, 3.9917726516723633, 6.767162799835205, 2.303173542022705, 3.9160985946655273, 4.113555908203125, 5.528571128845215, 1.9854751825332642, 2.307849645614624, 4.503940582275391, 4.5060648918151855, 2.9749650955200195, 4.258078098297119, 4.5211663246154785, 2.20393705368042, 4.5036940574646, 2.8892807960510254, 1.9136854410171509, 3.9147071838378906, 5.514667987823486, 4.193127632141113, 4.8884382247924805, 5.806581020355225, 3.3625378608703613, 5.739326477050781, 3.2708072662353516, 4.371150493621826, 4.850096702575684, 2.3916049003601074, 5.927370548248291, 6.318638801574707, 5.080280303955078, 1.4480963945388794, 2.3965797424316406, 6.4528303146362305, 2.5447795391082764, 3.541940689086914, 5.181678771972656, 3.4501023292541504, 4.178741931915283, 6.24529504776001, 3.540982723236084, 3.927248477935791, 3.7028841972351074, 3.6610541343688965, 5.04384708404541, 3.661862850189209, 4.096259117126465, 4.854657173156738, 4.485202312469482, 3.3703155517578125, 2.622070789337158, 4.840838432312012, 3.159564733505249, 6.832197189331055, 6.757671356201172, 4.966573238372803, 1.4242414236068726, 4.985146522521973, 2.907769203186035, 3.0692567825317383, 2.657701015472412, 1.7391788959503174, 3.9555325508117676, 3.36663818359375, 3.048715591430664, 2.306675434112549, 1.6899962425231934, 3.360779285430908, 6.185926914215088, 3.96366024017334, 4.004000186920166, 3.5575919151306152, 1.9165540933609009, 1.1348813772201538, 6.789395332336426, 4.15919303894043, 2.6044130325317383, 1.3429771661758423, 3.652817726135254, 5.081222057342529, 4.338277339935303, 2.1333227157592773, 4.4915242195129395, 4.484808921813965, 4.197126388549805, 3.7221670150756836, 2.132155656814575, 1.1908105611801147, 3.6419320106506348, 2.576226234436035, 7.129035949707031, 6.793662071228027, 4.6655802726745605, 3.9607315063476562, 4.970708847045898, 5.104825973510742, 3.8840999603271484, 2.7015738487243652, 5.241563320159912, 3.5858688354492188, 5.163061618804932, 4.836664199829102, 5.645822048187256, 4.689971446990967, 4.113325119018555, 4.201979160308838, 6.9386115074157715, 6.613006114959717, 4.769403457641602, 3.685053825378418, 2.654057741165161, 4.124993324279785, 4.615874767303467, 5.239755630493164, 4.128554821014404, 4.634018898010254, 4.351228713989258, 2.029874801635742, 3.9879956245422363, 4.474153518676758, 3.858713150024414, 5.416497230529785, 6.692569255828857, 4.565280437469482, 1.6367852687835693, 1.492730736732483, 3.510242462158203, 6.947333335876465, 4.112526893615723, 3.596309185028076, 4.4039530754089355, 6.375864028930664, 2.729626178741455, 5.802872657775879, 4.568517208099365, 4.447790145874023, 4.453248023986816, 5.069467544555664, 3.637497901916504, 3.2350692749023438, 3.1930525302886963, 1.9709832668304443, 6.162784576416016, 3.332396984100342, 4.717906951904297, 4.418558597564697, 6.973050117492676, 7.118635177612305, 2.0608370304107666, 2.6122329235076904, 5.021326541900635, 3.096428394317627, 4.131348133087158, 5.702130317687988, 3.1056394577026367, 5.239149570465088, 6.713438510894775, 5.153409481048584, 3.5853376388549805, 3.562854290008545, 6.32282829284668, 2.0936288833618164, 2.9089534282684326, 1.8158812522888184, 3.824368953704834, 4.390436172485352, 6.925665855407715, 2.254451274871826, 2.8103551864624023, 4.37224006652832, 4.117406845092773, 3.345602512359619, 4.800238132476807, 2.358079195022583, 2.540842056274414, 3.786677360534668, 2.7538819313049316, 1.349518895149231, 5.309520721435547, 3.531047821044922, 2.223893880844116, 5.865143775939941, 3.5269083976745605, 4.063390254974365, 5.39469051361084, 3.851785659790039, 4.108869552612305, 4.161505222320557, 3.454659938812256, 3.044893503189087, 2.9892144203186035, 3.495504856109619, 4.43300724029541, 3.4888768196105957]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train():\n",
    "    # Model parameters\n",
    "    g_input_size = 1      # Random noise dimension coming into generator, per output vector\n",
    "    g_hidden_size = 5     # Generator complexity\n",
    "    g_output_size = 1     # Size of generated output vector\n",
    "    d_input_size = 500    # Minibatch size - cardinality of distributions\n",
    "    d_hidden_size = 10    # Discriminator complexity\n",
    "    d_output_size = 1     # Single dimension for 'real' vs. 'fake' classification\n",
    "    minibatch_size = d_input_size\n",
    "\n",
    "    d_learning_rate = 1e-3\n",
    "    g_learning_rate = 1e-3\n",
    "    sgd_momentum = 0.9\n",
    "\n",
    "    num_epochs = 5000\n",
    "    print_interval = 100\n",
    "    d_steps = 20\n",
    "    g_steps = 20\n",
    "\n",
    "    dfe, dre, ge = 0, 0, 0\n",
    "    d_real_data, d_fake_data, g_fake_data = None, None, None\n",
    "\n",
    "    discriminator_activation_function = torch.sigmoid\n",
    "    generator_activation_function = torch.tanh\n",
    "\n",
    "    d_sampler = get_distribution_sampler(data_mean, data_stddev)\n",
    "    gi_sampler = get_generator_input_sampler()\n",
    "    G = Generator(input_size=g_input_size,\n",
    "                  hidden_size=g_hidden_size,\n",
    "                  output_size=g_output_size,\n",
    "                  f=generator_activation_function)\n",
    "    D = Discriminator(input_size=d_input_func(d_input_size),\n",
    "                      hidden_size=d_hidden_size,\n",
    "                      output_size=d_output_size,\n",
    "                      f=discriminator_activation_function)\n",
    "    criterion = nn.BCELoss()  # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
    "    d_optimizer = optim.SGD(D.parameters(), lr=d_learning_rate, momentum=sgd_momentum)\n",
    "    g_optimizer = optim.SGD(G.parameters(), lr=g_learning_rate, momentum=sgd_momentum)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for d_index in range(d_steps):\n",
    "            # 1. Train D on real+fake\n",
    "            D.zero_grad()\n",
    "\n",
    "            #  1A: Train D on real\n",
    "            d_real_data = Variable(d_sampler(d_input_size))\n",
    "            d_real_decision = D(preprocess(d_real_data))\n",
    "            d_real_error = criterion(d_real_decision, Variable(torch.ones([1,1])))  # ones = true\n",
    "            d_real_error.backward() # compute/store gradients, but don't change params\n",
    "\n",
    "            #  1B: Train D on fake\n",
    "            d_gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
    "            d_fake_data = G(d_gen_input).detach()  # detach to avoid training G on these labels\n",
    "            d_fake_decision = D(preprocess(d_fake_data.t()))\n",
    "            d_fake_error = criterion(d_fake_decision, Variable(torch.zeros([1,1])))  # zeros = fake\n",
    "            d_fake_error.backward()\n",
    "            d_optimizer.step()     # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
    "\n",
    "            dre, dfe = extract(d_real_error)[0], extract(d_fake_error)[0]\n",
    "\n",
    "        for g_index in range(g_steps):\n",
    "            # 2. Train G on D's response (but DO NOT train D on these labels)\n",
    "            G.zero_grad()\n",
    "\n",
    "            gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
    "            g_fake_data = G(gen_input)\n",
    "            dg_fake_decision = D(preprocess(g_fake_data.t()))\n",
    "            g_error = criterion(dg_fake_decision, Variable(torch.ones([1,1])))  # Train G to pretend it's genuine\n",
    "\n",
    "            g_error.backward()\n",
    "            g_optimizer.step()  # Only optimizes G's parameters\n",
    "            ge = extract(g_error)[0]\n",
    "\n",
    "        if epoch % print_interval == 0:\n",
    "            print(\"Epoch %s: D (%s real_err, %s fake_err) G (%s err); Real Dist (%s),  Fake Dist (%s) \" %\n",
    "                  (epoch, dre, dfe, ge, stats(extract(d_real_data)), stats(extract(d_fake_data))))\n",
    "\n",
    "    if matplotlib_is_available:\n",
    "        print(\"Plotting the generated distribution...\")\n",
    "        values = extract(g_fake_data)\n",
    "        print(\" Values: %s\" % (str(values)))\n",
    "        plt.hist(values, bins=50)\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Histogram of Generated Distribution')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
