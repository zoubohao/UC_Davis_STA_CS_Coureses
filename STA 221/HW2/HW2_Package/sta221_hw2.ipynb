{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 221HW1\n",
    "**Bohao Zou**  917796070  \n",
    "**Bingdao Chen**  917781027  \n",
    "**date**: 05/12/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pledge:__  \n",
    "Please sign below (print full name) after checking the following. If you can not honestly check each of these responses, please email me at kbala@ucdavis.edu to explain your situation.  \n",
    "\n",
    "We pledge that we are honest students with academic integrity and we have not cheated on this homework.\n",
    "\n",
    "These answers are our own work.\n",
    "\n",
    "We did not give any other students assistance on this homework.\n",
    "\n",
    "We understand that to submit work that is not our own and pretend that it is our is a violation of the UC Davis code of conduct and will be reported to Student Judicial Affairs.\n",
    "\n",
    "We understand that suspected misconduct on this homework will be reported to the Office of Student Support and Judicial Affairs and, if established, will result in disciplinary sanctions up through Dismissal from the University and a grade penalty up to a grade of ``F\" for the course.\n",
    "\n",
    "\n",
    "\n",
    "Team Member 1:  Bingdao Chen  \n",
    "Team Member 2:  Bohao Zou\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Answer true or false for each of the question below and give justification. \n",
    "\n",
    "(a) Gradient descent is a supervised learning algorithm. \n",
    "\n",
    "(b) Gradient descent is an algorithm to minimize or maximize a function.  \n",
    "\n",
    "(c) Logistic regression cannot be performed after linear PCA.  \n",
    "\n",
    "(d) Support vector machine is non-linear classification algorithm.  \n",
    "\n",
    "(e) A regression algorithm can be modified to be used for classification as well.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "(a) False. Supervised Learning consists of two problems: Regression and Classfication. Gradient descent is an optimization algorithm used to find the minimizer of functions.\n",
    "\n",
    "(b) True. Gradient descent is an algorithm to minimize a function. For the maximization problem of function $f(x)$, we can transfer it to find the minimizer of the $-f(x)$.\n",
    "\n",
    "(c) False. PCA is to reduce the dimentionality of the data by coming up with linear combinations of the variables that maximize the variance explained. Logistic regression can also be performed on thses newly masde variables.\n",
    "\n",
    "(d) False. Support vector machine is linear classification algorithm, because it creates a line or a hyperplane which separates the data into classes. However, SVM with kernel trick is a non-linear classification for mapping the data into the high-dimensional space.\n",
    "\n",
    "(e) True. We can add some functions to the output of regression like sigmoid or softmax to transform the regression problem to classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qustion 2\n",
    "### Solution\n",
    "#### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Kullback-Leibler (KL) divergence** is defined as \n",
    "$$\n",
    "D_{KL}(P\\|Q) = \\mathbb{E}_{x\\sim P}\\left[log\\frac{P(x)}{Q(x)}\\right] = \\mathbb{E}_{x\\sim P}\\left[logP(x)-logQ(x)\\right]\n",
    "$$\n",
    "It is used to measure how diﬀerent two distributions if there are two separate probability distributions $P(x)$ and $Q(x)$ over the same random variablex. Therein, the KL divergence is non-negative and asymmetry, so the sequence of distributions is important. For the issue that using a distribution $q(x)$ to approximate the distribution $p(x)$, the results are different if minimizing $D_{KL}(p\\|q)$ and $D_{KL}(q\\|p)$. For the first situation, $q(x)$ are more likely to  put high probability mass on all of modes of $p(x)$. For the second situation, $q(x)$ is more focused on one single mode of $p(x)$, avoiding putting probability mass in the low probability areas between modes of $p(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an unkonwn data-generating distribution $p_{data}(x)$, we want to find a good estimator $p_{model}(x;\\theta)$ for the true probability $p_{data}(x)$. Maximum Likelihood Estimation (MLE) is one method, and it is defined as \n",
    "$$\n",
    "\\theta_{MLE} = \\mathrm{argmax}_{\\theta}\\prod_{i=1}^m p_{model}(x^{(i)};\\theta)\n",
    "$$\n",
    "After log-transformation and rescaling, it can be expressed as an expectation with respect to the empirical distribution $\\hat p_{data}$:\n",
    "$$\n",
    "\\theta_{MLE} = \\mathrm{argmax}_{\\theta}\\mathbb{E}_{x\\sim\\hat p_{data}}\\mathrm{log}p_{model}(x;\\theta)\n",
    "$$\n",
    "Besides finding the maximum likelihood $\\theta_{MLE}$, another way to view this problem is to minimize the dissimilarity between the empirical distribution $\\hat p_{data}$ and the model distribution $p_{model}$, which can be measured by KL divergence as follows.\n",
    "$$\n",
    "D_{KL}(\\hat p_{data}\\| p_{model}) = \\mathbb{E}_{x\\sim\\hat p_{data}}\\left[\\mathrm{log}\\hat p_{data}(x) - \\mathrm{log}p_{model}(x) \\right]\n",
    "$$\n",
    "Considering $\\mathrm{log}\\hat p_{data}(x)$ is finxed, so this problem is equivalent to minimizing $-\\mathbb{E}_{x\\sim\\hat p_{data}}[\\mathrm{log}p_{model}(x)]$, which is same as the above MLE equation.\n",
    "\n",
    "In order to obtain a good estimator, we can either maximize the likelihood or minimize KL divergence. However, in practice, minimizing KL divergence is more desired for its non-negative property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some issues with explicit density models:\n",
    "\n",
    "(1) Long running time: For instance, FVBNs cannot generate samples in parallel, whose runtime is proportional to the dimensionality of $x$.\n",
    "\n",
    "(2) Too many restrictions: For instance, only a few probability distributions adimt tractable Markov chain sampling in Boltzmann machines. Generator must be invertible and the latent code $z$ must have the same dimension as the samples $x$ in non-linear ICA.\n",
    "\n",
    "(3) The need of Markov chains: For instance, Boltzmann machines and GSNs.\n",
    "\n",
    "(4) The need of variational bound and asymptotically consistent: For instance, some VAEs, but it is not yet proven.\n",
    "\n",
    "(5) The quality of samples produced by explicit density models is worse than GANs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea behind GANs:\n",
    "\n",
    "The framework of GANs can be interpreted as a game between two players. One is called discriminator and one is called generator. The discriminator examines samples from generator whether they are real or fake, and the generator is trained to fool the discriminator. In this process, the generator can learn to creat samples that are drawn from the same distribution as the training data.\n",
    "\n",
    "Formally, GANs contains latent variables $z$ and observed variables $x$. Function $D$ and $G$ stands for the discriminator and generator and use $\\theta^{(D)}$ and $\\theta^{(G)}$ as parameters respectively. The purpose of both two is to minimize its own cost fucntion $J^{(D)}(\\theta^{(D)},\\theta^{(G)})$ and $J^{(G)}(\\theta^{(D)},\\theta^{(G)})$ respectively. Both of them can only control its own parameters, and during this process, the solution is called Nash equilibrium which is $(\\theta^{(D)},\\theta^{(G)})$. It is a local minimum of $J^{(D)}$ with respect to $\\theta^{(D)}$ and a local minimum $J^{(G)}$ with respect to $\\theta^{(G)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three drawbacks for GANs:\n",
    "\n",
    "(1) It takes a lot time to train and needs huge computation.\n",
    "\n",
    "(2) Training a GAN need to find a Nash equilibrium of a game. However, sometimes gradient descent can do this, sometimes it can not. We do not have a good algorithem for finding equilibrium.\n",
    "\n",
    "(3) It’s hard to learn to generate discrete data, like text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "#### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train and test data\n",
    "house_train_data = pd.read_table('./housing_train.txt',sep= '\\s+', header = None)\n",
    "house_train_data.columns = [\"x1\", \"x2\", \"x3\", \"x4\", \"x5\",\"x6\",\"x7\",\"x8\",\"x9\",\"x10\",\"x11\",\"x12\",\"x13\",\"y\"]\n",
    "house_test_data = pd.read_table('./housing_test.txt',sep= '\\s+', header = None)\n",
    "house_test_data.columns = [\"x1\", \"x2\", \"x3\", \"x4\", \"x5\",\"x6\",\"x7\",\"x8\",\"x9\",\"x10\",\"x11\",\"x12\",\"x13\",\"y\"]\n",
    "\n",
    "# data pre-processing\n",
    "y_tr = house_train_data['y'].values\n",
    "X_tr = house_train_data.drop(['y'],axis=1).values\n",
    "y_te = house_test_data['y'].values\n",
    "X_te = house_test_data.drop(['y'],axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean square prediction error based on train data is 24.475882784643677.\n"
     ]
    }
   ],
   "source": [
    "# fit the model on train data and predict \n",
    "lrg = LinearRegression(fit_intercept = False).fit(X_tr, y_tr)\n",
    "\n",
    "# mean square prediction error on train data \n",
    "mspe_tr = np.mean((y_tr - lrg.predict(X_tr))**2)\n",
    "print(\"The mean square prediction error based on train data is \" + str(mspe_tr) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean square prediction error based on test data is 24.292238175660525.\n"
     ]
    }
   ],
   "source": [
    "# mean square prediction error\n",
    "mspe_te = np.mean((y_te - lrg.predict(X_te))**2)\n",
    "print(\"The mean square prediction error based on test data is \" + str(mspe_te) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean square prediction error based on the new train data is 5.431632129079359.\n",
      "The mean square prediction error based on the new test data is 34.68642969650256.\n"
     ]
    }
   ],
   "source": [
    "# obtain all X features \n",
    "X_poly_tr = PolynomialFeatures(degree = 2, include_bias = False).fit_transform(X_tr)\n",
    "X_poly_te = PolynomialFeatures(degree = 2, include_bias = False).fit_transform(X_te)\n",
    "\n",
    "# fit the model on the new train data \n",
    "lrg_poly = LinearRegression(fit_intercept = False).fit(X_poly_tr, y_tr)\n",
    "\n",
    "# mean square prediction error on the new train data \n",
    "mspe_poly_tr = np.mean((y_tr - lrg_poly.predict(X_poly_tr))**2)\n",
    "print(\"The mean square prediction error based on the new train data is \" + str(mspe_poly_tr) + \".\")\n",
    "\n",
    "# mean square prediction error on the new test data \n",
    "mspe_poly_te = np.mean((y_te - lrg_poly.predict(X_poly_te))**2)\n",
    "print(\"The mean square prediction error based on the new test data is \" + str(mspe_poly_te) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with the previous model, the mean squared prediction error of the polynomial linear regression model on train data is smaller, which indicates that it fits better. However,  the mean squared prediction error of the polynomial linear regression model on test data gets greater, which indicates that the polynomial model results in overfitting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "#### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data \n",
    "Amazon = pyreadr.read_r('Amazon.RData') \n",
    "df = Amazon[\"dat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rating value 1, there are 656 reviewers.\n",
      "For rating value 5, there are 656 reviewers.\n"
     ]
    }
   ],
   "source": [
    "np.unique(df[\"rating\"])\n",
    "print(\"For rating value 1, there are \" +  str(sum(df[\"rating\"]==1)) + \" reviewers.\")\n",
    "print(\"For rating value 5, there are \" +  str(sum(df[\"rating\"]==5)) + \" reviewers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performance of a constant classifier is that assigning all the reviews to one class which has the highest probability in the data. In this case, because the number of rating $= 5$ and rating $= 1$ are same, the misclassfication rate is 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat the Document-Term matrix and TF-IDF matrix\n",
    "text = df.loc[:,\"review\"]\n",
    "\n",
    "stemmer =FrenchStemmer()\n",
    "analyzer= CountVectorizer().build_analyzer()\n",
    "def stemmed_words(doc): \n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1312, 5186)\n",
      "(1312, 5186)\n"
     ]
    }
   ],
   "source": [
    "# Doucument term matrix generation\n",
    "vec = CountVectorizer(token_pattern = '[a-z]{3,15}', analyzer = stemmed_words)\n",
    "Xdtm = vec.fit_transform(text)\n",
    "dtm_name = vec.get_feature_names()\n",
    "dtm = pd.DataFrame(Xdtm.toarray(), columns = vec.get_feature_names())\n",
    "print(dtm.shape)\n",
    "\n",
    "# Tf-idf matrix generation\n",
    "vectorizer = TfidfVectorizer(token_pattern = '[a-z]{3,15}', analyzer = stemmed_words)\n",
    "Xtf = vectorizer.fit_transform(text)\n",
    "tf_name = vectorizer.get_feature_names()\n",
    "Tfidf = pd.DataFrame(Xtf.toarray(), columns = vectorizer.get_feature_names())\n",
    "print(Tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the data \n",
    "Xdtm_tr, Xdtm_te, ydtm_tr, ydtm_te = train_test_split(dtm, df['rating'], test_size=0.3, random_state=99)\n",
    "Xtf_tr, Xtf_te, ytf_tr, ytf_te = train_test_split(Tfidf, df['rating'], test_size=0.3, random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the regularization penalty is comprised of the sum of the absolute value of the coefficients, we need to scale the data such that its coefficients are all based on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the data \n",
    "sc_dtm = StandardScaler()\n",
    "Xdtm_tr_sd = sc_dtm.fit_transform(Xdtm_tr)\n",
    "Xdtm_te_sd = sc_dtm.transform(Xdtm_te)\n",
    "\n",
    "sc_tf = StandardScaler()\n",
    "Xtf_tr_sd = sc_tf.fit_transform(Xtf_tr)\n",
    "Xtf_te_sd = sc_tf.transform(Xtf_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a L1-regularized logistic regression model\n",
    "dtm_lr = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "dtm_lr.fit(Xdtm_tr_sd, ydtm_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_lr = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "tf_lr.fit(Xtf_tr_sd, ytf_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 426 covariates have non-zero coefficients in Document-Term Matrix.\n",
      "There are 457 covariates have non-zero coefficients in Tf-idf Matrix.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are \" + str(sum(dtm_lr.coef_[0] != 0)) + \n",
    "      \" covariates have non-zero coefficients in Document-Term Matrix.\")\n",
    "print(\"There are \" + str(sum(tf_lr.coef_[0] != 0)) + \n",
    "      \" covariates have non-zero coefficients in Tf-idf Matrix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True]\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True]\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True]\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "# verify that the most largest 20 coefficients are positive and the most least 20 coefficients are negative\n",
    "print(dtm_lr.coef_[0][dtm_lr.coef_[0].argsort()[::-1][:20]] > 0)\n",
    "print(dtm_lr.coef_[0][dtm_lr.coef_[0].argsort()[:20]] < 0)\n",
    "print(tf_lr.coef_[0][tf_lr.coef_[0].argsort()[::-1][:20]] > 0)\n",
    "print(tf_lr.coef_[0][tf_lr.coef_[0].argsort()[:20]] < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Document-Term Matrix, the twenty words with the most positive coefficients are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['lov', 'teething', 'soph', 'loved', 'voic', 'easy', 'great', 'she',\n",
       "       'highly', 'teeth', 'gift', 'chew', 'hear', 'seen', 'sensit',\n",
       "       'sony', 'channel', 'excellent', 'littl', 'awesom'], dtype='<U20')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(dtm_name)[dtm_lr.coef_[0].argsort()[::-1][:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Document-Term Matrix, the twenty words with the most negative coefficients are twenty words are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['not', 'leak', 'returned', 'something', 'out', 'wast', 'off',\n",
       "       'stopped', 'after', 'useless', 'get', 'back', 'return', 'even',\n",
       "       'bin', 'becaus', 'doesn', 'disappointed', 'leaked', 'pictur'],\n",
       "      dtype='<U20')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(dtm_name)[dtm_lr.coef_[0].argsort()[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Tf-idf Matrix, the twenty words with the most positive coefficients are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['lov', 'teething', 'soph', 'loved', 'she', 'voic', 'great',\n",
       "       'highly', 'gift', 'toy', 'easy', 'sensit', 'littl', 'channel',\n",
       "       'teeth', 'other', 'her', 'confined', 'hear', 'sony'], dtype='<U20')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tf_name)[tf_lr.coef_[0].argsort()[::-1][:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Tf-idf Matrix, the twenty words with the most negative coefficients are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['not', 'wast', 'off', 'out', 'diap', 'after', 'leak', 'bottl',\n",
       "       'the', 'swing', 'wrap', 'horribl', 'motorol', 'return', 'beeping',\n",
       "       'something', 'doesn', 'you', 'returned', 'pictur'], dtype='<U20')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tf_name)[tf_lr.coef_[0].argsort()[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The misclassification rate of Document-Term model is 0.07360406091370558.\n",
      "The misclassification rate of Tf-idf model is 0.06598984771573604.\n"
     ]
    }
   ],
   "source": [
    "print(\"The misclassification rate of Document-Term model is \" + str(statistics.mean(ydtm_te != dtm_lr.predict(Xdtm_te_sd)))+ \".\")\n",
    "print(\"The misclassification rate of Tf-idf model is \" + str(statistics.mean(ytf_te != tf_lr.predict(Xtf_te_sd)))+ \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.47969543147208127\n",
      "1\n",
      "0.47969543147208127\n"
     ]
    }
   ],
   "source": [
    "# misclassification rate for constant classifier\n",
    "dummy_dtm = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_dtm.fit(Xdtm_te, ydtm_te)\n",
    "print(dummy_dtm.predict(Xdtm_te)[0])\n",
    "print(1-dummy_dtm.score(Xdtm_te, ydtm_te))\n",
    "\n",
    "dummy_tf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_tf.fit(Xtf_te, ytf_te)\n",
    "print(dummy_tf.predict(Xtf_te)[0])\n",
    "print(1-dummy_tf.score(Xtf_te, ytf_te))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, the misclassification rate of both model are very low. However, for the constant classifier, because it ignores the content of reviews and blindly assigns all reviews to one class, the results of document term matrix and Tf-idf matrix are same. It assigns all the review to the \"bad\" class, and the misclassfication rate is 0.4797, which is much higher than the misclassfication rate of the first model. The logisitic model is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
