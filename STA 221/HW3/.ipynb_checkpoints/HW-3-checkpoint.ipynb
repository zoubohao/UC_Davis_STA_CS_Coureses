{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW-3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr\n",
    "import torch\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a data set for this hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HW3_Data_set(Dataset):\n",
    "\n",
    "    def __init__(self,dataset,dataset_labels):\n",
    "        super().__init__()\n",
    "        self.set = torch.from_numpy(dataset)\n",
    "        self.set_labels = torch.from_numpy(dataset_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.set_labels)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.set[item] , self.set_labels[item]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf(dataset, vector,sigma):\n",
    "    \"\"\"\n",
    "    :param dataset: [n,d]\n",
    "    :param vector: [b,d]\n",
    "    :param sigma : rbf sigma, must be positive\n",
    "    :return: [b,n]\n",
    "    \"\"\"\n",
    "    b = vector.shape[0]\n",
    "    d = vector.shape[1]\n",
    "    vectors = torch.chunk(vector,dim=0,chunks=b)\n",
    "    batchDis = []\n",
    "    for oneVec in vectors:\n",
    "        # print(\"dataset\",dataset)\n",
    "        # print(\"x\",oneVec)\n",
    "        # print(dataset - oneVec.view(1,d))\n",
    "        dis = torch.sum(torch.pow(dataset - oneVec.view(1,d), 2.), dim=1, keepdim=False)\n",
    "        batchDis.append(dis)\n",
    "    #print(\"before div sigma\",-torch.stack(batchDis,dim=0))\n",
    "    #print(\"After div sigma\", -torch.stack(batchDis,dim=0) / (2. * sigma))\n",
    "    #print(\"Final result \" , torch.exp(-torch.stack(batchDis,dim=0) / (2. * sigma)))\n",
    "    return torch.exp(-torch.stack(batchDis,dim=0) / (2. * sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(nn.Module):\n",
    "\n",
    "    def __init__(self,data_set, data_labels,sigma):\n",
    "        \"\"\"\n",
    "        :param data_set: [n,d]\n",
    "        :param data_labels: 1,-1 set [n]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_set = torch.from_numpy(data_set).float()\n",
    "        self.data_labels = torch.from_numpy(data_labels).float()\n",
    "        self.n = self.data_set.size()[0]\n",
    "        self.lambdas = nn.Parameter(torch.ones(size=[self.n],requires_grad=True).float(),requires_grad=True)\n",
    "        self.b = nn.Parameter(torch.ones(1,requires_grad=True).float(),requires_grad=True)\n",
    "        self.sigma2 = np.square(sigma)\n",
    "\n",
    "    def forward(self,x):\n",
    "        ## return [b]\n",
    "        lambdas = F.relu(self.lambdas)\n",
    "        result = self.data_labels.view(1,self.n) * lambdas.view(1,self.n) * rbf(self.data_set,x,self.sigma2)\n",
    "        #print(\"Lambda \",lambdas)\n",
    "        #print(\"Before sign \",torch.sum(result,dim=1) + self.b)\n",
    "        return torch.tanh(torch.sum(result,dim=1) + self.b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training function for Logistic regression and Neural net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,lossCri,dataLoader,testDataLoader,epoch,weight_decay,lr,device = \"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(),lr=lr,weight_decay=weight_decay,nesterov=True,momentum=0.9)\n",
    "    lossCri = lossCri.to(device)\n",
    "    for e in range(epoch):\n",
    "        finalLoss = 0\n",
    "        for times, (trainRows, trainLabels) in enumerate(dataLoader):\n",
    "            trainRows = trainRows.float().to(device)\n",
    "            trainLabels = trainLabels.long().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            trainOutput = model(trainRows)\n",
    "            loss = lossCri(trainOutput,trainLabels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            finalLoss = loss\n",
    "        model = model.eval()\n",
    "        # Calculate Accuracy\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # Iterate through test dataset\n",
    "        for testRows, testLabels in testDataLoader:\n",
    "            testLabels = testLabels.to(device)\n",
    "            # Forward pass only to get logits/output\n",
    "            testOutputs = model(testRows.float().to(device))\n",
    "\n",
    "            # Get predictions from the maximum value\n",
    "            _, predicted = testOutputs.max(1)\n",
    "\n",
    "            # Total number of labels\n",
    "            total += testLabels.size(0)\n",
    "\n",
    "            # Total correct predictions\n",
    "            correct += predicted.eq(testLabels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        # Print Loss\n",
    "        print('Epoch: {}. Loss : {}.  Accuracy In Test Data set: {}'.format(e,finalLoss, accuracy))\n",
    "        model = model.train(True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training function for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(model,dataLoader,testDataLoader,epoch,lr,weight_decay,device = \"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(),lr=lr,weight_decay=weight_decay,nesterov=True,momentum=0.9)\n",
    "    lossCri = nn.MSELoss().to(device)\n",
    "    for e in range(epoch):\n",
    "        for times, (trainRows, trainLabels) in enumerate(dataLoader):\n",
    "            #print(times)\n",
    "            trainRows = trainRows.float().to(device)\n",
    "            trainLabels = trainLabels.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            trainOutput = model(trainRows)\n",
    "            loss = lossCri(trainOutput,trainLabels)\n",
    "            # print(\"##########\")\n",
    "            # print(trainOutput)\n",
    "            # print(trainLabels)\n",
    "            # print(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model = model.eval()\n",
    "        # Calculate Accuracy\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # Iterate through test dataset\n",
    "        for testRows, testLabels in testDataLoader:\n",
    "            testLabels = testLabels.to(device)\n",
    "            # Forward pass only to get logits/output\n",
    "            testOutputs = model(testRows.float().to(device))\n",
    "\n",
    "            # Get predictions from the maximum value\n",
    "            b = testOutputs.size(0)\n",
    "            predicted = torch.where(testOutputs >= 0.,torch.ones(size=[b]),torch.zeros(size=[b])-1)\n",
    "            total += testLabels.size(0)\n",
    "\n",
    "            # Total correct predictions\n",
    "            correct += predicted.eq(testLabels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        # Print Result\n",
    "        print('Epoch: {}.  Accuracy In Test Data set: {}%'.format(e, accuracy))\n",
    "        model = model.train(True)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pyreadr.read_r(\"./DeepFeature.RData\")\n",
    "\n",
    "\n",
    "deep_features = np.transpose(np.array(data[\"deep.feature\"]).reshape([-1,2000]))\n",
    "image_array = np.transpose(np.array(data[\"image.array\"]).reshape([-1,2000]))\n",
    "labelsChar = np.array(data[\"label\"]).reshape([2000])\n",
    "labels01 = np.array(labelsChar == \"cat\",dtype=np.float32)\n",
    "labels1_1 = np.where(labelsChar == \"cat\",-1,1)\n",
    "\n",
    "\n",
    "dp_train, dp_test, dp_label_train, dp_label_test = \\\n",
    "        train_test_split(deep_features, labels01, test_size=0.30)\n",
    "\n",
    "ia_train, ia_test, ia_label_train, ia_label_test = \\\n",
    "        train_test_split(image_array, labels01, test_size=0.30)\n",
    "\n",
    "dp_D = dp_train.shape[1]\n",
    "ia_D = ia_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct data set and data Loader For Logistic regression and Neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train dataset\n",
    "dp_train_set = HW3_Data_set(dataset=dp_train,dataset_labels=dp_label_train)\n",
    "ia_train_set = HW3_Data_set(dataset=ia_train / 255.,dataset_labels=ia_label_train)\n",
    "\n",
    "## test dataset\n",
    "dp_test_set = HW3_Data_set(dataset=dp_test,dataset_labels=dp_label_test)\n",
    "ia_test_set = HW3_Data_set(dataset=ia_test / 255.,dataset_labels=ia_label_test)\n",
    "\n",
    "## data loader\n",
    "dp_train_loader = DataLoader(dp_train_set,batch_size=25,shuffle=True)\n",
    "ia_train_loader = DataLoader(ia_train_set,batch_size=25,shuffle=True)\n",
    "\n",
    "dp_test_loader = DataLoader(dp_test_set,batch_size=25,shuffle=True)\n",
    "ia_test_loader = DataLoader(ia_test_set,batch_size=25,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic regression deep.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression for deep.feature\n",
      "Epoch: 0. Loss : 0.618233323097229.  Accuracy In Test Data set: 69.66666666666667\n",
      "Epoch: 1. Loss : 0.5216636061668396.  Accuracy In Test Data set: 73.5\n",
      "Epoch: 2. Loss : 0.43095821142196655.  Accuracy In Test Data set: 72.5\n",
      "Epoch: 3. Loss : 0.40423375368118286.  Accuracy In Test Data set: 73.83333333333333\n",
      "Epoch: 4. Loss : 0.4658602178096771.  Accuracy In Test Data set: 74.16666666666667\n",
      "Epoch: 5. Loss : 0.30143672227859497.  Accuracy In Test Data set: 74.16666666666667\n",
      "Epoch: 6. Loss : 0.42810672521591187.  Accuracy In Test Data set: 75.0\n",
      "Epoch: 7. Loss : 0.42620915174484253.  Accuracy In Test Data set: 73.83333333333333\n",
      "Epoch: 8. Loss : 0.38976600766181946.  Accuracy In Test Data set: 75.66666666666667\n",
      "Epoch: 9. Loss : 0.32068726420402527.  Accuracy In Test Data set: 76.33333333333333\n",
      "Epoch: 10. Loss : 0.5471965670585632.  Accuracy In Test Data set: 76.5\n",
      "Epoch: 11. Loss : 0.40021923184394836.  Accuracy In Test Data set: 76.16666666666667\n",
      "Epoch: 12. Loss : 0.368092805147171.  Accuracy In Test Data set: 77.33333333333333\n",
      "Epoch: 13. Loss : 0.28653669357299805.  Accuracy In Test Data set: 75.0\n",
      "Epoch: 14. Loss : 0.3928617835044861.  Accuracy In Test Data set: 76.16666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(\n",
       "  (linear): Linear(in_features=4096, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Logistic regression for deep.feature\")\n",
    "logisticR = LogisticRegression(dp_D,2)\n",
    "lossCriL = nn.CrossEntropyLoss()\n",
    "train(logisticR,lossCriL,dp_train_loader,dp_test_loader,epoch=15,weight_decay=5e-4,lr= 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic regression image.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistric regression for image.array\n",
      "Epoch: 0. Loss : 0.6404321193695068.  Accuracy In Test Data set: 54.0\n",
      "Epoch: 1. Loss : 0.7539743781089783.  Accuracy In Test Data set: 53.5\n",
      "Epoch: 2. Loss : 0.6664098501205444.  Accuracy In Test Data set: 53.833333333333336\n",
      "Epoch: 3. Loss : 0.6730889678001404.  Accuracy In Test Data set: 54.5\n",
      "Epoch: 4. Loss : 0.7043527364730835.  Accuracy In Test Data set: 56.0\n",
      "Epoch: 5. Loss : 0.6156076192855835.  Accuracy In Test Data set: 55.333333333333336\n",
      "Epoch: 6. Loss : 0.6986448764801025.  Accuracy In Test Data set: 56.5\n",
      "Epoch: 7. Loss : 0.6669578552246094.  Accuracy In Test Data set: 57.166666666666664\n",
      "Epoch: 8. Loss : 0.6634848117828369.  Accuracy In Test Data set: 57.5\n",
      "Epoch: 9. Loss : 0.6242170929908752.  Accuracy In Test Data set: 57.333333333333336\n",
      "Epoch: 10. Loss : 0.6415241360664368.  Accuracy In Test Data set: 57.333333333333336\n",
      "Epoch: 11. Loss : 0.7325840592384338.  Accuracy In Test Data set: 53.0\n",
      "Epoch: 12. Loss : 0.5978148579597473.  Accuracy In Test Data set: 58.333333333333336\n",
      "Epoch: 13. Loss : 0.6981485486030579.  Accuracy In Test Data set: 56.166666666666664\n",
      "Epoch: 14. Loss : 0.664691686630249.  Accuracy In Test Data set: 56.333333333333336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(\n",
       "  (linear): Linear(in_features=3072, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Logistric regression for image.array\")\n",
    "logisticR = LogisticRegression(ia_D,2)\n",
    "lossCriL = nn.CrossEntropyLoss()\n",
    "train(logisticR,lossCriL,ia_train_loader,ia_test_loader,epoch=15,weight_decay=5e-4,lr= 0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "svm data loader construct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### svm data loader construct\n",
    "dp_train_svm, dp_test_svm, dp_label_train_svm, dp_label_test_svm = \\\n",
    "        train_test_split(deep_features, labels1_1, test_size=0.30)\n",
    "\n",
    "ia_train_svm, ia_test_svm, ia_label_train_svm, ia_label_test_svm = \\\n",
    "        train_test_split(image_array, labels1_1, test_size=0.30)\n",
    "\n",
    "## train dataset\n",
    "dp_train_set_svm = HW3_Data_set(dataset=dp_train_svm,dataset_labels=dp_label_train_svm)\n",
    "ia_train_set_svm = HW3_Data_set(dataset=ia_train_svm / 255.,dataset_labels=ia_label_train_svm)\n",
    "\n",
    "## test dataset\n",
    "dp_test_set_svm = HW3_Data_set(dataset=dp_test_svm,dataset_labels=dp_label_test_svm)\n",
    "ia_test_set_svm = HW3_Data_set(dataset=ia_test_svm / 255.,dataset_labels=ia_label_test_svm)\n",
    "\n",
    "## data loader\n",
    "dp_train_loader_svm = DataLoader(dp_train_set_svm,batch_size=1,shuffle=True)\n",
    "ia_train_loader_svm = DataLoader(ia_train_set_svm,batch_size=1,shuffle=True)\n",
    "\n",
    "dp_test_loader_svm = DataLoader(dp_test_set_svm,batch_size=1,shuffle=False)\n",
    "ia_test_loader_svm = DataLoader(ia_test_set_svm,batch_size=1,shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM for deep.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM for deep.feature.\n",
      "Epoch: 0.  Accuracy In Test Data set: 74.5%\n",
      "Epoch: 1.  Accuracy In Test Data set: 75.5%\n",
      "Epoch: 2.  Accuracy In Test Data set: 75.16666666666667%\n",
      "Epoch: 3.  Accuracy In Test Data set: 75.0%\n",
      "Epoch: 4.  Accuracy In Test Data set: 75.33333333333333%\n",
      "Epoch: 5.  Accuracy In Test Data set: 75.16666666666667%\n",
      "Epoch: 6.  Accuracy In Test Data set: 75.5%\n",
      "Epoch: 7.  Accuracy In Test Data set: 76.0%\n",
      "Epoch: 8.  Accuracy In Test Data set: 75.16666666666667%\n",
      "Epoch: 9.  Accuracy In Test Data set: 74.33333333333333%\n",
      "Epoch: 10.  Accuracy In Test Data set: 75.83333333333333%\n",
      "Epoch: 11.  Accuracy In Test Data set: 75.83333333333333%\n",
      "Epoch: 12.  Accuracy In Test Data set: 75.66666666666667%\n",
      "Epoch: 13.  Accuracy In Test Data set: 74.5%\n",
      "Epoch: 14.  Accuracy In Test Data set: 75.16666666666667%\n",
      "Epoch: 15.  Accuracy In Test Data set: 74.5%\n",
      "Epoch: 16.  Accuracy In Test Data set: 74.83333333333333%\n",
      "Epoch: 17.  Accuracy In Test Data set: 74.83333333333333%\n",
      "Epoch: 18.  Accuracy In Test Data set: 74.83333333333333%\n",
      "Epoch: 19.  Accuracy In Test Data set: 75.83333333333333%\n",
      "Epoch: 20.  Accuracy In Test Data set: 74.83333333333333%\n",
      "Epoch: 21.  Accuracy In Test Data set: 75.0%\n",
      "Epoch: 22.  Accuracy In Test Data set: 75.83333333333333%\n",
      "Epoch: 23.  Accuracy In Test Data set: 75.66666666666667%\n",
      "Epoch: 24.  Accuracy In Test Data set: 75.16666666666667%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVM()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"SVM for deep.feature.\")\n",
    "svm = SVM(dp_train_svm,dp_label_train_svm,sigma=36)\n",
    "train_svm(svm,dp_train_loader_svm,dp_test_loader_svm,epoch=25,lr = 0.0001,weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM for image.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM for image.array.\n",
      "Epoch: 0.  Accuracy In Test Data set: 55.833333333333336%\n",
      "Epoch: 1.  Accuracy In Test Data set: 58.666666666666664%\n",
      "Epoch: 2.  Accuracy In Test Data set: 58.833333333333336%\n",
      "Epoch: 3.  Accuracy In Test Data set: 58.666666666666664%\n",
      "Epoch: 4.  Accuracy In Test Data set: 56.166666666666664%\n",
      "Epoch: 5.  Accuracy In Test Data set: 59.0%\n",
      "Epoch: 6.  Accuracy In Test Data set: 58.833333333333336%\n",
      "Epoch: 7.  Accuracy In Test Data set: 59.0%\n",
      "Epoch: 8.  Accuracy In Test Data set: 59.166666666666664%\n",
      "Epoch: 9.  Accuracy In Test Data set: 59.333333333333336%\n",
      "Epoch: 10.  Accuracy In Test Data set: 59.5%\n",
      "Epoch: 11.  Accuracy In Test Data set: 58.666666666666664%\n",
      "Epoch: 12.  Accuracy In Test Data set: 57.0%\n",
      "Epoch: 13.  Accuracy In Test Data set: 58.666666666666664%\n",
      "Epoch: 14.  Accuracy In Test Data set: 59.166666666666664%\n",
      "Epoch: 15.  Accuracy In Test Data set: 59.333333333333336%\n",
      "Epoch: 16.  Accuracy In Test Data set: 58.666666666666664%\n",
      "Epoch: 17.  Accuracy In Test Data set: 57.0%\n",
      "Epoch: 18.  Accuracy In Test Data set: 59.5%\n",
      "Epoch: 19.  Accuracy In Test Data set: 58.833333333333336%\n",
      "Epoch: 20.  Accuracy In Test Data set: 56.0%\n",
      "Epoch: 21.  Accuracy In Test Data set: 56.5%\n",
      "Epoch: 22.  Accuracy In Test Data set: 59.0%\n",
      "Epoch: 23.  Accuracy In Test Data set: 57.0%\n",
      "Epoch: 24.  Accuracy In Test Data set: 58.833333333333336%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVM()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"SVM for image.array.\")\n",
    "svm = SVM(ia_train_svm / 255.,ia_label_train_svm,sigma=18)\n",
    "train_svm(svm,ia_train_loader_svm,ia_test_loader_svm,epoch=25,lr = 0.0001,weight_decay=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural net for deep.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net for deep.feature\n",
      "Epoch: 0. Loss : 0.6050152778625488.  Accuracy In Test Data set: 64.83333333333333\n",
      "Epoch: 1. Loss : 0.5596890449523926.  Accuracy In Test Data set: 70.0\n",
      "Epoch: 2. Loss : 0.5768333077430725.  Accuracy In Test Data set: 71.33333333333333\n",
      "Epoch: 3. Loss : 0.4804919958114624.  Accuracy In Test Data set: 72.66666666666667\n",
      "Epoch: 4. Loss : 0.5203630924224854.  Accuracy In Test Data set: 74.0\n",
      "Epoch: 5. Loss : 0.47613945603370667.  Accuracy In Test Data set: 74.16666666666667\n",
      "Epoch: 6. Loss : 0.4962328374385834.  Accuracy In Test Data set: 74.16666666666667\n",
      "Epoch: 7. Loss : 0.37645038962364197.  Accuracy In Test Data set: 75.16666666666667\n",
      "Epoch: 8. Loss : 0.627378523349762.  Accuracy In Test Data set: 75.0\n",
      "Epoch: 9. Loss : 0.5210436582565308.  Accuracy In Test Data set: 75.16666666666667\n",
      "Epoch: 10. Loss : 0.30259668827056885.  Accuracy In Test Data set: 75.33333333333333\n",
      "Epoch: 11. Loss : 0.48448115587234497.  Accuracy In Test Data set: 75.33333333333333\n",
      "Epoch: 12. Loss : 0.3672466576099396.  Accuracy In Test Data set: 75.66666666666667\n",
      "Epoch: 13. Loss : 0.5037244558334351.  Accuracy In Test Data set: 75.0\n",
      "Epoch: 14. Loss : 0.4227086305618286.  Accuracy In Test Data set: 75.33333333333333\n",
      "Epoch: 15. Loss : 0.3617876470088959.  Accuracy In Test Data set: 76.16666666666667\n",
      "Epoch: 16. Loss : 0.4731428027153015.  Accuracy In Test Data set: 75.33333333333333\n",
      "Epoch: 17. Loss : 0.29523172974586487.  Accuracy In Test Data set: 76.66666666666667\n",
      "Epoch: 18. Loss : 0.5299986004829407.  Accuracy In Test Data set: 74.5\n",
      "Epoch: 19. Loss : 0.4761286675930023.  Accuracy In Test Data set: 77.16666666666667\n",
      "Epoch: 20. Loss : 0.227852001786232.  Accuracy In Test Data set: 74.83333333333333\n",
      "Epoch: 21. Loss : 0.36396467685699463.  Accuracy In Test Data set: 76.83333333333333\n",
      "Epoch: 22. Loss : 0.33322039246559143.  Accuracy In Test Data set: 77.5\n",
      "Epoch: 23. Loss : 0.30610787868499756.  Accuracy In Test Data set: 77.16666666666667\n",
      "Epoch: 24. Loss : 0.39261317253112793.  Accuracy In Test Data set: 77.83333333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (fc1): Linear(in_features=4096, out_features=2000, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc3): Linear(in_features=2000, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Neural net for deep.feature\")\n",
    "NNt = NeuralNet(dp_D,hidden_size=2000,num_classes=2)\n",
    "lossCriL = nn.CrossEntropyLoss()\n",
    "train(NNt,lossCriL,dp_train_loader,dp_test_loader,epoch=25,weight_decay=5e-4,lr= 0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural net for image.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net for image.array\n",
      "Epoch: 0. Loss : 0.6889950633049011.  Accuracy In Test Data set: 51.0\n",
      "Epoch: 1. Loss : 0.6793661713600159.  Accuracy In Test Data set: 53.5\n",
      "Epoch: 2. Loss : 0.6936144828796387.  Accuracy In Test Data set: 54.833333333333336\n",
      "Epoch: 3. Loss : 0.7094132304191589.  Accuracy In Test Data set: 54.666666666666664\n",
      "Epoch: 4. Loss : 0.678005576133728.  Accuracy In Test Data set: 55.666666666666664\n",
      "Epoch: 5. Loss : 0.6816192865371704.  Accuracy In Test Data set: 55.166666666666664\n",
      "Epoch: 6. Loss : 0.653522789478302.  Accuracy In Test Data set: 55.0\n",
      "Epoch: 7. Loss : 0.6258469223976135.  Accuracy In Test Data set: 55.166666666666664\n",
      "Epoch: 8. Loss : 0.7234441637992859.  Accuracy In Test Data set: 55.333333333333336\n",
      "Epoch: 9. Loss : 0.674009382724762.  Accuracy In Test Data set: 56.0\n",
      "Epoch: 10. Loss : 0.6038647890090942.  Accuracy In Test Data set: 56.166666666666664\n",
      "Epoch: 11. Loss : 0.6934240460395813.  Accuracy In Test Data set: 55.333333333333336\n",
      "Epoch: 12. Loss : 0.6726822853088379.  Accuracy In Test Data set: 56.0\n",
      "Epoch: 13. Loss : 0.6721738576889038.  Accuracy In Test Data set: 56.5\n",
      "Epoch: 14. Loss : 0.6669750213623047.  Accuracy In Test Data set: 55.833333333333336\n",
      "Epoch: 15. Loss : 0.7005160450935364.  Accuracy In Test Data set: 57.0\n",
      "Epoch: 16. Loss : 0.6902008056640625.  Accuracy In Test Data set: 55.5\n",
      "Epoch: 17. Loss : 0.6385571956634521.  Accuracy In Test Data set: 57.333333333333336\n",
      "Epoch: 18. Loss : 0.6679340600967407.  Accuracy In Test Data set: 57.166666666666664\n",
      "Epoch: 19. Loss : 0.6707133650779724.  Accuracy In Test Data set: 56.333333333333336\n",
      "Epoch: 20. Loss : 0.7076117992401123.  Accuracy In Test Data set: 54.666666666666664\n",
      "Epoch: 21. Loss : 0.675130307674408.  Accuracy In Test Data set: 57.166666666666664\n",
      "Epoch: 22. Loss : 0.720598042011261.  Accuracy In Test Data set: 57.666666666666664\n",
      "Epoch: 23. Loss : 0.637239396572113.  Accuracy In Test Data set: 57.5\n",
      "Epoch: 24. Loss : 0.7447100281715393.  Accuracy In Test Data set: 57.833333333333336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (fc1): Linear(in_features=3072, out_features=2000, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc3): Linear(in_features=2000, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Neural net for image.array\")\n",
    "NNt = NeuralNet(ia_D,hidden_size=2000,num_classes=2)\n",
    "lossCriL = nn.CrossEntropyLoss()\n",
    "train(NNt,lossCriL,ia_train_loader,ia_test_loader,epoch=25,weight_decay=5e-4,lr= 0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
