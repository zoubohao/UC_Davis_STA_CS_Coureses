glmModel = glm(V9~V2+V3+V4+V5+V6+V8, family = binomial(), data = pima.data)
res.D = residuals(glmModel, type = "deviance")
res.P = residuals(glmModel, type = "pearson")
par(mfrow= c(1,3))
### goodness of fit
P_stat = sum(res.P^2)
D_stat = sum(res.D^2)
par(mfrow=c(1,3))
boxplot(cbind(res.P, res.D), names = c("Pearson", "Deviance"))
plot(glmModel$fitted.values, res.P, pch=16, cex=0.6, ylab='Pearson Residuals', xlab='Fitted Values')
lines(smooth.spline(glmModel$fitted.values, res.P, spar=2), col=2)
abline(h=0, lty=2, col='grey')
plot(glmModel$fitted.values, res.D, pch=16, cex=0.6, ylab='Deviance Residuals', xlab='Fitted Values')
lines(smooth.spline(glmModel$fitted.values, res.D, spar=2), col=2)
abline(h=0, lty=2, col='grey')
par(mfrow= c(1,3))
### goodness of fit
P_stat = sum(res.P^2)
D_stat = sum(res.D^2)
par(mfrow=c(1,3))
boxplot(cbind(res.P, res.D), names = c("Pearson", "Deviance"))
plot(glmModel$fitted.values, res.P, pch=16, cex=0.6, ylab='Pearson Residuals', xlab='Fitted Values')
lines(smooth.spline(glmModel$fitted.values, res.P, spar=2.5), col=2)
abline(h=0, lty=2, col='grey')
plot(glmModel$fitted.values, res.D, pch=16, cex=0.6, ylab='Deviance Residuals', xlab='Fitted Values')
lines(smooth.spline(glmModel$fitted.values, res.D, spar=2.5), col=2)
abline(h=0, lty=2, col='grey')
library(lawstat)
library(mgcv)
mgcvgam2 <- mgcv::gam(V9 ~ s(V2,bs="cr",k=15) + s(V3, bs="cr", k=15) +
s(V4,bs="cr",k=15) + s(V5,bs="cr",k=15) + s(V6,bs="cr",k=15) +
s(V8,bs="cr",k=15),
family=binomial(),data=pima.data)
# k specifies number of basis functions
# bs specifies the basis functions used here: cubic splines here; default is thin plate splines.
summary(mgcvgam2)
summary(glmModel)
outputTable = tidy(glmModel)
library(broom)
outputTable = tidy(glmModel)
for (i in c(2:dim(outputTable)[2])){
outputTable[,i] = round(outputTable[,i], 6)
}
write.csv(outputTable,"d:\\table.csv")
outputTable = tidy(mgcvgam2)
for (i in c(2:dim(outputTable)[2])){
outputTable[,i] = round(outputTable[,i], 6)
}
write.csv(outputTable,"d:\\table.csv")
plot(mgcvgam2)
plot(mgcvgam2)
plot(mgcvgam2, residuals=TRUE, shade = TRUE)
plot(mgcvgam2, residuals=TRUE, shade = TRUE)
mgcvgam2$fitted.values
plot(mgcvgam2, residuals=TRUE, shade = TRUE)
# k specifies number of basis functions
# bs specifies the basis functions used here: cubic splines here; default is thin plate splines.
summary(mgcvgam2)
library(gam)
gamgam1 <- gam::gam(V9 ~ s(V2,4) + s(V3,5) + v4 + v5 + s(v6,4) + s(v8,5),
family=binomial(), data=pima.data)
gamgam1 <- gam::gam(V9 ~ s(V2,4) + s(V3,5) + V4 + V5 + s(V6,4) + s(V8,5),
family=binomial(), data=pima.data)
# smoothing spline with df=4 for Height and df=5 for Girth.
summary(gamgam1)
# k specifies number of basis functions
# bs specifies the basis functions used here: cubic splines here; default is thin plate splines.
summary(mgcvgam2)
gamgam1 <- gam::gam(V9 ~ s(V2,4) + V3 + V4 + V5 + s(V6,4) + s(V8,5),
family=binomial(), data=pima.data)
# smoothing spline with df=4 for Height and df=5 for Girth.
summary(gamgam1)
# k specifies number of basis functions
# bs specifies the basis functions used here: cubic splines here; default is thin plate splines.
summary(mgcvgam2)
mgcvgam2 <- mgcv::gam(V9 ~ s(V2) + s(V3) +
s(V4) + s(V5) + s(V6) +
s(V8),
family=binomial(),data=pima.data)
# k specifies number of basis functions
# bs specifies the basis functions used here: cubic splines here; default is thin plate splines.
summary(mgcvgam2)
gamgam1 <- gam::gam(V9 ~ s(V2,4) + s(V3,5) + V4 + V5 + s(V6,4) + s(V8,5),
family=binomial(), data=pima.data)
# smoothing spline with df=4 for Height and df=5 for Girth.
summary(gamgam1)
plot(gamgam1)
outputTable = tidy(gamgam1)
for (i in c(2:dim(outputTable)[2])){
outputTable[,i] = round(outputTable[,i], 6)
}
write.csv(outputTable,"d:\\table.csv")
#library(cvTools)
library(boot)
?cv.glm
data(mammals, package="MASS")
mammals.glm <- glm(log(brain) ~ log(body), data = mammals)
# actually is a linear model regressing log(brain) on log(body) here
cv.err <- cv.glm(mammals, mammals.glm)$delta # leave-one-out CV
cv.err
cv.err.6 <- cv.glm(mammals, mammals.glm, K = 6)$delta # 6-fold CV
cv.err.6
head(mammals)
library(MASS)
?lda
head(iris)
table(iris$Species)
# n=150, p=4, N_s = N_c = N_v = 50
train <- sample(1:150, 75)
?glmnet # fit a model with a set of lambda
library(glmnet)
?glmnet # fit a model with a set of lambda
?cv.glmnet # K-fold cross validation for a saturated model to choose lambda
?cv.glm
glmPIMA <- glm(V9~., family = binomial(), data = pima.data)
cv.err.10 <- cv.glm(pima.data, glmPIMA, K = 10)$delta # 10-fold CV
cv.err.10
head(pima.data)
### quadratic
glmPIMAq <- glm(V9~V1^2 + V2^2 + V3^2 + V4^2 + V5^2 + V6^2
+ V7^2 + V8^2
, family = binomial(), data = pima.data)
cv.err.10 <- cv.glm(pima.data, glmPIMAq, K = 10)$delta # 10-fold CV
cv.err.10
?lda
outputTable[,i] = round(outputTable[,i], 6)
### Fisher linear dis ana
head(iris)
# n=150, p=4, N_s = N_c = N_v = 50
train <- sample(1:150, 75)
train
### Fisher linear dis ana
n = dim(pima.data)[1]
n
nums = round(n/10)
nums
a = c(1:10)
b = c(2,3)
setdiff(a,b)
TenCV = function(data,k = 10){
n = dim(data)[1]
nums = round(n/k)
wholeSet = c(1:n)
misVec = c()
for (i in c(1:k)){
test <- c(1*k: (1*k + nums))
train = setdiff(wholeSet, test)
z <- lda(V9 ~ ., data, subset = train)
pred <- predict(z, data[-train, ])$class
cl.test <- data$V9[-train]
table(pred, cl.test)
# To get the misclassification rate
misrate <- length(which(cl.test != pred))/length(cl.test)
misVec = c(misrate, misVec)
}
return(mean(misVec))
}
cv.lda.10 = TenCV(pima.data)
cv.lda.10
TenCV = function(data,k = 10){
n = dim(data)[1]
nums = round(n/k)
wholeSet = c(1:n)
misVec = c()
for (i in c(1:k)){
test <- c(1*k: (1*k + nums))
train = setdiff(wholeSet, test)
z <- qda(V9 ~ ., data, subset = train)
pred <- predict(z, data[-train, ])$class
cl.test <- data$V9[-train]
table(pred, cl.test)
# To get the misclassification rate
misrate <- length(which(cl.test != pred))/length(cl.test)
misVec = c(misrate, misVec)
}
return(mean(misVec))
}
### Q linear dis ana
TenCV = function(data,k = 10){
n = dim(data)[1]
nums = round(n/k)
wholeSet = c(1:n)
misVec = c()
for (i in c(1:k)){
test <- c(1*k: (1*k + nums))
train = setdiff(wholeSet, test)
z <- qda(V9 ~ ., data, subset = train)
pred <- predict(z, data[-train, ])$class
cl.test <- data$V9[-train]
table(pred, cl.test)
# To get the misclassification rate
misrate <- length(which(cl.test != pred))/length(cl.test)
misVec = c(misrate, misVec)
}
return(mean(misVec))
}
cv.qda.10 = TenCV(pima.data)
cv.qda.10
### Fisher linear dis ana
TenldaCV = function(data,k = 10){
n = dim(data)[1]
nums = round(n/k)
wholeSet = c(1:n)
misVec = c()
for (i in c(1:k)){
test <- c((1 + nums*(i-1)): (1 + nums*i))
train = setdiff(wholeSet, test)
z <- lda(V9 ~ ., data, subset = train)
pred <- predict(z, data[-train, ])$class
cl.test <- data$V9[-train]
table(pred, cl.test)
# To get the misclassification rate
misrate <- length(which(cl.test != pred))/length(cl.test)
misVec = c(misrate, misVec)
}
return(mean(misVec))
}
cv.lda.10 = TenCV(pima.data)
cv.lda.10
### Fisher linear dis ana
TenldaCV = function(data,k = 10){
n = dim(data)[1]
nums = round(n/k)
wholeSet = c(1:n)
misVec = c()
for (i in c(1:k)){
test <- c((1 + nums*(i-1)): (1 + nums*i))
print(test)
train = setdiff(wholeSet, test)
z <- lda(V9 ~ ., data, subset = train)
pred <- predict(z, data[-train, ])$class
cl.test <- data$V9[-train]
table(pred, cl.test)
# To get the misclassification rate
misrate <- length(which(cl.test != pred))/length(cl.test)
misVec = c(misrate, misVec)
}
return(mean(misVec))
}
cv.lda.10 = TenCV(pima.data)
cv.lda.10 = TenldaCV(pima.data)
### Fisher linear dis ana
TenldaCV = function(data,k = 10){
n = dim(data)[1]
nums = round(n/k)
wholeSet = c(1:n)
misVec = c()
for (i in c(1:k)){
test <- c((1 + nums*(i-1)): (nums*i))
print(test)
train = setdiff(wholeSet, test)
z <- lda(V9 ~ ., data, subset = train)
pred <- predict(z, data[-train, ])$class
cl.test <- data$V9[-train]
table(pred, cl.test)
# To get the misclassification rate
misrate <- length(which(cl.test != pred))/length(cl.test)
misVec = c(misrate, misVec)
}
return(mean(misVec))
}
cv.lda.10 = TenldaCV(pima.data)
cv.lda.10
pima.data = read.table("C:\\Users\\Admin\\Desktop\\BST 223\\HW6\\pima-indians-diabetes.txt",sep=",")
library(boot)
?cv.glm
### linear predictor
glmPIMA <- glm(V9~., family = binomial(), data = pima.data)
cv.err.10 <- cv.glm(pima.data, glmPIMA, K = 10)$delta # 10-fold CV
cv.err.10
### quadratic
glmPIMAq <- glm(V9~V1^2 + V2^2 + V3^2 + V4^2 + V5^2 + V6^2
+ V7^2 + V8^2
, family = binomial(), data = pima.data)
cv.err.10 <- cv.glm(pima.data, glmPIMAq, K = 10)$delta # 10-fold CV
cv.err.10
### Fisher linear dis ana
library(MASS)
TenldaCV = function(data,k = 10){
n = dim(data)[1]
nums = round(n/k)
wholeSet = c(1:n)
misVec = c()
for (i in c(1:k)){
test <- c((1 + nums*(i-1)): (nums*i))
print(test)
train = setdiff(wholeSet, test)
z <- lda(V9 ~ ., data, subset = train)
pred <- predict(z, data[-train, ])$class
cl.test <- data$V9[-train]
# To get the misclassification rate
misrate <- length(which(cl.test != pred))/length(cl.test)
misVec = c(misrate, misVec)
}
return(mean(misVec))
}
cv.lda.10 = TenldaCV(pima.data)
cv.lda.10
### Q linear dis ana
TenqdaCV = function(data,k = 10){
n = dim(data)[1]
nums = round(n/k)
wholeSet = c(1:n)
misVec = c()
for (i in c(1:k)){
test <- c((1 + nums*(i-1)): (nums*i))
print(test)
train = setdiff(wholeSet, test)
z <- qda(V9 ~ ., data, subset = train)
pred <- predict(z, data[-train, ])$class
cl.test <- data$V9[-train]
# To get the misclassification rate
misrate <- length(which(cl.test != pred))/length(cl.test)
misVec = c(misrate, misVec)
}
return(mean(misVec))
}
cv.qda.10 = TenCV(pima.data)
cv.qda.10 = TenqdaCV(pima.data)
cv.qda.10
### Q linear dis ana
TenqdaCV = function(data,k = 10){
n = dim(data)[1]
nums = round(n/k)
wholeSet = c(1:n)
misVec = c()
for (i in c(1:k)){
test <- c((1 + nums*(i-1)): (nums*i))
train = setdiff(wholeSet, test)
print(train)
z <- qda(V9 ~ ., data, subset = train)
pred <- predict(z, data[-train, ])$class
cl.test <- data$V9[-train]
# To get the misclassification rate
misrate <- length(which(cl.test != pred))/length(cl.test)
misVec = c(misrate, misVec)
}
return(mean(misVec))
}
cv.qda.10 = TenqdaCV(pima.data)
TenldaCV = function(data,k = 10){
n = dim(data)[1]
nums = round(n/k)
wholeSet = c(1:n)
misVec = c()
for (i in c(1:k)){
test <- c((1 + nums*(i-1)): (nums*i))
train = setdiff(wholeSet, test)
z <- lda(V9 ~ ., data, subset = train)
pred <- predict(z, data[-train, ])$class
cl.test <- data$V9[-train]
# To get the misclassification rate
misrate <- length(which(cl.test != pred))/length(cl.test)
misVec = c(misrate, misVec)
}
return(mean(misVec))
}
cv.lda.10 = TenldaCV(pima.data)
cv.lda.10
### Q linear dis ana
TenqdaCV = function(data,k = 10){
n = dim(data)[1]
nums = round(n/k)
wholeSet = c(1:n)
misVec = c()
for (i in c(1:k)){
test <- c((1 + nums*(i-1)): (nums*i))
train = setdiff(wholeSet, test)
z <- qda(V9 ~ ., data, subset = train)
pred <- predict(z, data[-train, ])$class
cl.test <- data$V9[-train]
# To get the misclassification rate
misrate <- length(which(cl.test != pred))/length(cl.test)
misVec = c(misrate, misVec)
}
return(mean(misVec))
}
cv.qda.10 = TenqdaCV(pima.data)
cv.qda.10
### linear predictor
glmPIMA <- glm(V9~., family = binomial(), data = pima.data)
cv.err.10 <- cv.glm(pima.data, glmPIMA, K = 10)$delta # 10-fold CV
cv.err.10
### quadratic
glmPIMAq <- glm(V9~V1^2 + V2^2 + V3^2 + V4^2 + V5^2 + V6^2
+ V7^2 + V8^2
, family = binomial(), data = pima.data)
cv.err.10 <- cv.glm(pima.data, glmPIMAq, K = 10)$delta # 10-fold CV
cv.err.10
### Fisher linear dis ana
library(MASS)
TenldaCV = function(data,k = 10){
n = dim(data)[1]
nums = round(n/k)
wholeSet = c(1:n)
misVec = c()
for (i in c(1:k)){
test <- c((1 + nums*(i-1)): (nums*i))
train = setdiff(wholeSet, test)
z <- lda(V9 ~ ., data, subset = train)
pred <- predict(z, data[-train, ])$class
cl.test <- data$V9[-train]
# To get the misclassification rate
misrate <- length(which(cl.test != pred))/length(cl.test)
misVec = c(misrate, misVec)
}
return(mean(misVec))
}
cv.lda.10 = TenldaCV(pima.data)
cv.lda.10
### Q linear dis ana
TenqdaCV = function(data,k = 10){
n = dim(data)[1]
nums = round(n/k)
wholeSet = c(1:n)
misVec = c()
for (i in c(1:k)){
test <- c((1 + nums*(i-1)): (nums*i))
train = setdiff(wholeSet, test)
z <- qda(V9 ~ ., data, subset = train)
pred <- predict(z, data[-train, ])$class
cl.test <- data$V9[-train]
# To get the misclassification rate
misrate <- length(which(cl.test != pred))/length(cl.test)
misVec = c(misrate, misVec)
}
return(mean(misVec))
}
cv.qda.10 = TenqdaCV(pima.data)
cv.qda.10
chemo.data = read.table("C:\\Users\\Admin\\Desktop\\BST 223\\HW3\\chemo.txt")
count = chemo.data$V2 + chemo.data$V3 + chemo.data$V4 + chemo.data$V5
newData = data.frame(count = count, baseline = chemo.data$V7, age = chemo.data$V8,
treatment = as.factor(chemo.data$V6))
plot(x = c(1:59), y = count,pch=16, cex=0.6, xlab = "index", ylab = "count number", main = "Scatter Plot")
outindex = which(count>250)
cleanData = newData[-49,]
head(cleanData)
head(chemo.data$V6)
head(cleanChemoData)
cleanChemoData = newData[-49,]
head(cleanChemoData)
n = dim(cleanChemoData)[1]
model = glm(count~baseline + age + treatment, family = poisson(), data = cleanData)
model$deviance / (n-length(coef(model)))
?glm.nb
?zeroinfl
library(pscl)
?zeroinfl
# Zero-Inflated Poisson Model
ziPoiss <- zeroinfl(count~baseline + age + treatment|baseline + age + treatment,
data = cleanChemoData, dist = 'poisson')
# Zero-Inflated Negative Binomial Model
zinb <- zeroinfl(count~baseline + age + treatment|baseline + age + treatment,
data = cleanChemoData, dist = 'negbin')
summary(zinb)
summary(ziPoiss)
outputTable = tidy(poissonModel)
poissonModel = glm(count~baseline + age + treatment, family = poisson(), data = cleanData)
outputTable = tidy(poissonModel)
for (i in c(2:dim(outputTable)[2])){
outputTable[,i] = round(outputTable[,i], 6)
}
write.csv(outputTable,"d:\\table.csv")
outputTable = tidy(nbModel)
nbModel <- glm.nb(count~baseline + age + treatment, data=cleanChemoData)
outputTable = tidy(nbModel)
outputTable
summary(nbModel)
for (i in c(2:dim(outputTable)[2])){
outputTable[,i] = round(outputTable[,i], 6)
}
write.csv(outputTable,"d:\\table.csv")
# Zero-Inflated Poisson Model
ziPoiss <- zeroinfl(count~baseline + age + treatment|baseline + age + treatment,
data = cleanChemoData, dist = 'poisson')
outputTable = tidy(ziPoiss)
summary(ziPoiss)
# Zero-Inflated Negative Binomial Model
zinb <- zeroinfl(count~baseline + age + treatment|baseline + age + treatment,
data = cleanChemoData, dist = 'negbin')
summary(zinb)
1 - pchisq(as.numeric(2*(logLik(ziPoiss) - logLik(poissonModel))),
df = length(coef(ziPoiss)) - length(coef(poissonModel)))
length(coef(ziPoiss))
length(coef(poissonModel))
vuong(poissonModel,ziPoiss)
vuong(nbModel,zinb)
1 - pchisq(as.numeric(2*(logLik(ziPoiss) - logLik(poissonModel))),
df = length(coef(ziPoiss)) - length(coef(poissonModel)))
poissonModel$deviance / (n-length(coef(poissonModel)))
nbModel$deviance / (n-length(coef(nbModel)))
?vcoc
?vcov
?weights
weights(poissonModel)
weights(nbModel)
diag(weights(nbModel))
weightMatrix = diag(weights(nbModel))
dim(weightMatrix)
vcov(nbModel)
beta = coef(nbModel)
beta
rnorm(0)
rnorm(1)
?rnonm
?qnorm
summary(nbModel)
vcov(nbModel)
sqrt
sqrt(0.1707858346)
sqrt(1.228523e-05)
